INFO: To setup environment run:
      eval "$(/apps/languages/anaconda/2024.02/bin/conda shell.bash hook)"
  or just:
      source activate
/bin/sh: sox: command not found
SoX could not be found!

    If you do not have SoX, proceed here:
     - - - http://sox.sourceforge.net/ - - -

    If you do (or think that you should) have SoX, double-check your
    path variables.
    
[rank: 0] Global seed set to 1234
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: hulmeed (hulmeed-cardiff-university). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in data/wandb/run-20241017_165852-bq929372
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run MERT-v1-95M
wandb: â­ï¸ View project at https://wandb.ai/hulmeed-cardiff-university/Eval_EMO_probing
wandb: ğŸš€ View run at https://wandb.ai/hulmeed-cardiff-university/Eval_EMO_probing/runs/bq929372
/home/c.c22091860/.conda/envs/marble/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
Using 16bit None Automatic Mixed Precision (AMP)
/home/c.c22091860/.conda/envs/marble/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/native_amp.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home/c.c22091860/.conda/envs/marble/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(

   | Name             | Type    | Params
----------------------------------------------
0  | hidden_0         | Linear  | 393 K 
1  | output           | Linear  | 1.0 K 
2  | dropout          | Dropout | 0     
3  | loss             | MSELoss | 0     
4  | train_r2         | R2Score | 0     
5  | train_arousal_r2 | R2Score | 0     
6  | train_valence_r2 | R2Score | 0     
7  | valid_r2         | R2Score | 0     
8  | valid_arousal_r2 | R2Score | 0     
9  | valid_valence_r2 | R2Score | 0     
10 | test_r2          | R2Score | 0     
11 | test_arousal_r2  | R2Score | 0     
12 | test_valence_r2  | R2Score | 0     
----------------------------------------------
394 K     Trainable params
0         Non-trainable params
394 K     Total params
0.790     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
/home/c.c22091860/.conda/envs/marble/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/c.c22091860/.conda/envs/marble/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/c.c22091860/.conda/envs/marble/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Metric valid_r2 improved. New best score: 0.319
Metric valid_r2 improved by 0.146 >= min_delta = 0.0. New best score: 0.465
Metric valid_r2 improved by 0.081 >= min_delta = 0.0. New best score: 0.546
Metric valid_r2 improved by 0.027 >= min_delta = 0.0. New best score: 0.573
Metric valid_r2 improved by 0.025 >= min_delta = 0.0. New best score: 0.598
Metric valid_r2 improved by 0.013 >= min_delta = 0.0. New best score: 0.611
Metric valid_r2 improved by 0.006 >= min_delta = 0.0. New best score: 0.617
Metric valid_r2 improved by 0.009 >= min_delta = 0.0. New best score: 0.627
Metric valid_r2 improved by 0.001 >= min_delta = 0.0. New best score: 0.628
Metric valid_r2 improved by 0.001 >= min_delta = 0.0. New best score: 0.629
Metric valid_r2 improved by 0.002 >= min_delta = 0.0. New best score: 0.631
Metric valid_r2 improved by 0.001 >= min_delta = 0.0. New best score: 0.632
Metric valid_r2 improved by 0.003 >= min_delta = 0.0. New best score: 0.635
Metric valid_r2 improved by 0.000 >= min_delta = 0.0. New best score: 0.635
Metric valid_r2 improved by 0.000 >= min_delta = 0.0. New best score: 0.635
Monitored metric valid_r2 did not improve in the last 20 records. Best score: 0.635. Signaling Trainer to stop.
Using 16bit None Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Restoring states from the checkpoint path at ./lightning_logs/bq929372/checkpoints/epoch=41-step=336-valid_loss=0.3452-valid_r2=0.6351.ckpt
/home/c.c22091860/.conda/envs/marble/lib/python3.9/site-packages/lightning_fabric/utilities/cloud_io.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(f, map_location=map_location)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./lightning_logs/bq929372/checkpoints/epoch=41-step=336-valid_loss=0.3452-valid_r2=0.6351.ckpt
Restoring states from the checkpoint path at ./lightning_logs/bq929372/checkpoints/epoch=41-step=336-valid_loss=0.3452-valid_r2=0.6351.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from checkpoint at ./lightning_logs/bq929372/checkpoints/epoch=41-step=336-valid_loss=0.3452-valid_r2=0.6351.ckpt
/home/c.c22091860/.conda/envs/marble/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
wandb: - 0.018 MB of 0.018 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–
wandb:     test_arousal_r2 â–
wandb:           test_loss â–
wandb:             test_r2 â–
wandb:     test_valence_r2 â–
wandb:    train_arousal_r2 â–â–ƒâ–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          train_loss â–ˆâ–…â–„â–ƒâ–â–‚â–â–â–
wandb:            train_r2 â–â–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    train_valence_r2 â–â–„â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: trainer/global_step â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:    valid_arousal_r2 â–â–†â–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:          valid_loss â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:            valid_r2 â–â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    valid_valence_r2 â–â–„â–†â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:     test_arousal_r2 0.76845
wandb:           test_loss 0.28004
wandb:             test_r2 0.68501
wandb:     test_valence_r2 0.60157
wandb:    train_arousal_r2 0.86378
wandb:          train_loss 0.15529
wandb:            train_r2 0.8554
wandb:    train_valence_r2 0.84702
wandb: trainer/global_step 0
wandb:    valid_arousal_r2 0.72637
wandb:          valid_loss 0.34517
wandb:            valid_r2 0.63512
wandb:    valid_valence_r2 0.54387
wandb: 
wandb: ğŸš€ View run MERT-v1-95M at: https://wandb.ai/hulmeed-cardiff-university/Eval_EMO_probing/runs/bq929372
wandb: â­ï¸ View project at: https://wandb.ai/hulmeed-cardiff-university/Eval_EMO_probing
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: data/wandb/run-20241017_165852-bq929372/logs
